{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group information**\n",
    "\n",
    "| Family name | First name | Email address |\n",
    "| ----------- | ---------- | ------------- |\n",
    "|             |            |               |\n",
    "|             |            |               |\n",
    "|             |            |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language - Practice\n",
    "\n",
    "This tutorial examines the use of deep language models to predict the political affiliation of tweet authors, either Republican or Democrat. Using a labelled [dataset](https://www.kaggle.com/datasets/kapastor/democratvsrepublicantweets?resource=download) of approximately 75,000 tweets of varying lengths, the goal is to learn a function that maps textual input to the probability that a given tweet is authored by a Democrat. The modelling approach should explicitly capture both the semantic and syntactic structure of the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Large language models** can be powerful learning tools, but make sure to continue asking questions until you fully understand the produced answer and can judge its correctness. Simply copy-pasting generated code does not contribute to learning and is a waste of your class time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /opt/anaconda3/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from torchtext) (4.66.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchtext) (2.9.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (69.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torchtext) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install torchtext -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:38:21.039215Z",
     "iopub.status.busy": "2022-12-14T13:38:21.038725Z",
     "iopub.status.idle": "2022-12-14T13:38:23.412222Z",
     "shell.execute_reply": "2022-12-14T13:38:23.411516Z"
    },
    "id": "z682XYsrjkY9"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not load this library: /opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_ops.py:1488\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: <7E3C8144-0701-3505-8587-6E953627B6AF> /opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <072C24B6-780D-3534-8D91-63C0F4E85C20> /opt/anaconda3/lib/python3.12/site-packages/torch/lib/libc10.dylib",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_ops.py:1490\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1490\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load this library: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "\u001b[0;31mOSError\u001b[0m: Could not load this library: /opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import torch\n",
    "import torchinfo\n",
    "import torchtext\n",
    "import tqdm\n",
    "import umap\n",
    "\n",
    "from sklearn import metrics, model_selection\n",
    "from torch import nn, optim, utils\n",
    "from urllib import request\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def download_data():\n",
    "    if os.getcwd().endswith('/data'):\n",
    "        print('Data folder already exists')\n",
    "    else:\n",
    "        request.urlretrieve('https://www.dropbox.com/scl/fo/z31b6hon9625a02n19whg/AGGw2KPRyRk-qSDRVeQbutc?rlkey=t2ea3a3snykgu2y35l99rakbu&dl=1', 'data.zip')\n",
    "        shutil.unpack_archive('data.zip', 'data')\n",
    "        os.remove('data.zip')\n",
    "        os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute on first run\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Descriptive statistics**\n",
    "\n",
    "Load the `dataset.csv` file, display a sample of tweets along with their corresponding party labels. Compute the frequency distribution of the labels. Using `torchtext.data.utils.get_tokenizer`, tokenise the tweets. Identify the most common hashtags for each party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Build vocabulary**\n",
    "\n",
    "Load pre-trained GloVe embeddings using `torchtext.vocab.GloVe`. Build the vocabulary using `torchtext.vocab.build_vocab_from_iterator`, keeping tokens that appear at least five times and limiting the vocabulary to the 10,000 most frequent. Report the match rate as well as examples of matched and unmatched tokens.\n",
    "\n",
    "Note: For efficiency, only the subset of tokens present in the vocabulary is retained from the 27B-token dictionary. To download the full dataset, delete the cached file `glove.twitter.27B.100d.txt` or specify a different cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads pre-trained embeddings\n",
    "embeds = torchtext.vocab.GloVe(name='twitter.27B', dim=100, cache='.')\n",
    "vocab  = torchtext.vocab.build_vocab_from_iterator(iterator=dataset['tokens'], min_freq=5, specials=['<pad>', '<unk>'], max_tokens=10_000)\n",
    "vocab.set_default_index(vocab['<unk>']) # Unmatched words are assigned the \"unknown\" token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Data loaders**\n",
    "\n",
    "Map tokens to their corresponding vocabulary indices and encode party labels as `0.0` (Republican) and `1.0` (Democrat). Split the data into training (80%) and test (20%) sample. Using the provided `TweetDataset` and `collate_fn`, initialise training and test `utils.data.DataLoader` instances with `batch_size=128`, enabling shuffling for the training loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def collate_fn(batch, pad_index=vocab['<pad>']):\n",
    "    X, y = zip(*batch)\n",
    "    X = [torch.tensor(x, dtype=torch.long) for x in X]\n",
    "    X = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=pad_index)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Model structure**\n",
    "\n",
    "Create an embedding layer that maps token indices to embedding vectors using `torch.nn.Embedding.from_pretrained`, with `freeze=True` to keep the embeddings fixed during training.\n",
    "\n",
    "Define a PyTorch model class that maps a token sequence of shape $b \\times t \\times d$ to a scalar score using an embedding layer, a 32-unit bidirectional LSTM encoder, dropout regularisation, and a single-unit output layer with no activation. For numerical stability, PyTorch loss functions expect raw logit scores rather than a probability. The sigmoid transformation is applied internally within the loss function. \n",
    "\n",
    "Instantiate the model with a dropout probability of $0.5$, as these models quickly overfit, and print its structure using `torchinfo.summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding_layer = torch.nn.Embedding.from_pretrained(\n",
    "    embeddings=embeds.get_vecs_by_tokens(vocab.get_itos()),\n",
    "    freeze=True,\n",
    "    padding_idx=vocab['<pad>']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Model training** \n",
    "\n",
    "Define the appropriate loss function `nn.BCEWithLogitsLoss` and an optimisation algorithm (e.g. `optim.AdamW`).  Write a PyTorch training loop to estimate the model parameters using the training sample, with a maximum of 25 epochs and a learning rate of `1e-3`. Remember to move the model and the batch data to the correct device.\n",
    "\n",
    "Optionally, implement an early-stopping procedure on the validation sample to monitor generalisation performance across epochs, stop training when overfitting begins, and restore the parameters that perform best out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Threshold selection**\n",
    "\n",
    "Write a prediction loop for the validation sample and, using the predicted probabilities, select the ROC-based decision threshold that maximises the true positive rate while minimising the false positive rate. Given the class balance in the training sample, what threshold value would you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Model performance** \n",
    "\n",
    "Apply the prediction loop to the test set, use the selected threshold, and evaluate performance with `metrics.classification_report` and `metrics.confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Prediction**\n",
    "\n",
    "Display the tweets with the highest and lowest predicted Democrat probability, as well as those with the most neutral scores. Write a pipeline to test the model on your own custom input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Sequence representations**\n",
    "\n",
    "The LSTM layer outputs numerical sequence representations that serve as tweet embeddings.  Extract these embeddings and, for a selected tweet, identify the most similar tweets using cosine similarity. How do these representations compare with those obtained via singular value decomposition? What do you think these distances reflect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS (intermediate)** \n",
    "\n",
    "Project the tweet representations into a two- or three-dimensional space using a dimensionality-reduction method such as UMAP. Colour points by predicted probability and distinguish correct and incorrect predictions by marker symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS (advanced)** \n",
    "\n",
    "Using `captum.attr.LayerIntegratedGradients`, compute token-level importance by averaging attribution scores with respect to the modelâ€™s predictions, either for a single instance or across the full dataset. Identify the most influential words, as well as those most predictive of Democratic and Republican classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
