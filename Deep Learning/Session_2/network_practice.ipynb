{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group information**\n",
    "\n",
    "| Family name | First name | Email address |\n",
    "| ----------- | ---------- | ------------- |\n",
    "|             |            |               |\n",
    "|             |            |               |\n",
    "|             |            |               |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network - Practice\n",
    "\n",
    "This tutorial explores how to implement a simple neural network to predict the likelihood of loan default from borrower loan characteristics. The labelled dataset contains 100,000 observations and 16 predictors (e.g. income, credit score). The response is a binary variable indicating whether the borrower defaulted on the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:58:34.784086Z",
     "start_time": "2026-01-19T14:58:31.973221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\r\n",
      "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from captum) (3.10.0)\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/anaconda3/lib/python3.12/site-packages (from captum) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from captum) (25.0)\r\n",
      "Requirement already satisfied: torch>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from captum) (2.9.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from captum) (4.66.4)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (69.5.1)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10->captum) (2024.3.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.10->captum) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10->captum) (2.1.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\r\n",
      "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: captum\r\n",
      "Successfully installed captum-0.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:58:47.593121Z",
     "start_time": "2026-01-19T14:58:44.933536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "from captum import attr\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from torch import nn, optim, utils\n",
    "from tqdm import tqdm\n",
    "from urllib import request\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "# Utilities\n",
    "def download_data():\n",
    "    '''Downloads the data folder'''\n",
    "    if os.getcwd().endswith('/data'):\n",
    "        print('Data folder already exists')\n",
    "    else:\n",
    "        request.urlretrieve('https://www.dropbox.com/scl/fo/tniycpagp0c3p72uy0ag1/ACdmVyp71Zw_89tPERPN2mI?rlkey=0nxq0gifiqh5fwl9j0dk8lgk9&dl=1', 'data.zip')\n",
    "        shutil.unpack_archive('data.zip', 'data')\n",
    "        os.remove('data.zip')\n",
    "        os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T14:58:55.686776Z",
     "start_time": "2026-01-19T14:58:49.486093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute on first run\n",
    "download_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Descriptive statistics**\n",
    "\n",
    "Load the `X.csv` and `y.csv` files using `pd.read_csv`. Display the first few observations with the `head` method and generate descriptive statistics for both continuous (e.g. `describe`) and categorical (e.g. `value_counts`) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:04:23.619748Z",
     "start_time": "2026-01-19T15:04:23.541194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eduardo/Desktop/Applied-Data-Science-Classes/data\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:05:44.005504Z",
     "start_time": "2026-01-19T15:05:43.543210Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('X.csv')\n",
    "y = pd.read_csv('y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:09:06.948343Z",
     "start_time": "2026-01-19T15:09:06.869682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loan_id  age  income  loan_amount  credit_score  months_employed  \\\n",
      "0        0   29   17239       104883           769               93   \n",
      "1        1   19   72980        51678           752               42   \n",
      "2        2   59   31220       223085           550               22   \n",
      "3        3   39   85614       213354           817               97   \n",
      "4        4   61  149367        94041           535              117   \n",
      "\n",
      "   num_credit_lines  interest_rate  loan_term  dti_ratio    education  \\\n",
      "0                 4           2.10         24       0.82     bachelor   \n",
      "1                 3          10.95         24       0.25  high_school   \n",
      "2                 2          21.02         60       0.66  high_school   \n",
      "3                 3          13.59         36       0.32  high_school   \n",
      "4                 4          19.25         36       0.52          phd   \n",
      "\n",
      "  employment_type marital_status has_mortgage has_dependents loan_purpose  \\\n",
      "0       part_time       divorced          yes            yes     business   \n",
      "1   self_employed         single           no            yes    education   \n",
      "2       full_time        married           no             no         home   \n",
      "3       part_time        married           no            yes         auto   \n",
      "4   self_employed       divorced          yes             no         auto   \n",
      "\n",
      "  has_cosigner  \n",
      "0          yes  \n",
      "1           no  \n",
      "2           no  \n",
      "3          yes  \n",
      "4           no  \n",
      "             loan_id            age         income    loan_amount  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean    49999.500000      42.117450   85538.004260  120137.762090   \n",
      "std     28867.657797      15.035917   38550.745757   71200.016682   \n",
      "min         0.000000      18.000000   15004.000000    5000.000000   \n",
      "25%     24999.750000      29.000000   53211.250000   57636.500000   \n",
      "50%     49999.500000      41.000000   86744.500000  116466.000000   \n",
      "75%     74999.250000      55.000000  118891.250000  180868.750000   \n",
      "max     99999.000000      69.000000  149997.000000  249998.000000   \n",
      "\n",
      "        credit_score  months_employed  num_credit_lines  interest_rate  \\\n",
      "count  100000.000000    100000.000000     100000.000000  100000.000000   \n",
      "mean      572.537830        57.643000          2.519270      13.971571   \n",
      "std       159.037166        34.670781          1.119304       6.652417   \n",
      "min       300.000000         0.000000          1.000000       2.000000   \n",
      "25%       435.000000        27.000000          2.000000       8.280000   \n",
      "50%       572.000000        57.000000          3.000000      14.180000   \n",
      "75%       710.000000        88.000000          4.000000      19.810000   \n",
      "max       849.000000       119.000000          4.000000      25.000000   \n",
      "\n",
      "           loan_term      dti_ratio  \n",
      "count  100000.000000  100000.000000  \n",
      "mean       35.972640       0.503031  \n",
      "std        16.966213       0.230766  \n",
      "min        12.000000       0.100000  \n",
      "25%        24.000000       0.300000  \n",
      "50%        36.000000       0.500000  \n",
      "75%        48.000000       0.700000  \n",
      "max        60.000000       0.900000  \n",
      "   loan_id  default\n",
      "0        0        1\n",
      "1        1        0\n",
      "2        2        1\n",
      "3        3        0\n",
      "4        4        0\n",
      "             loan_id        default\n",
      "count  100000.000000  100000.000000\n",
      "mean    49999.500000       0.285170\n",
      "std     28867.657797       0.451498\n",
      "min         0.000000       0.000000\n",
      "25%     24999.750000       0.000000\n",
      "50%     49999.500000       0.000000\n",
      "75%     74999.250000       1.000000\n",
      "max     99999.000000       1.000000\n"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(X.describe())\n",
    "print(y.head())\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:10:35.035638Z",
     "start_time": "2026-01-19T15:10:34.985075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- education ---\n",
      "education\n",
      "high_school    25698\n",
      "bachelor       25392\n",
      "master         24706\n",
      "phd            24204\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- employment_type ---\n",
      "employment_type\n",
      "unemployed       25966\n",
      "part_time        25269\n",
      "self_employed    24706\n",
      "full_time        24059\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- marital_status ---\n",
      "marital_status\n",
      "divorced    33966\n",
      "single      33221\n",
      "married     32813\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- has_mortgage ---\n",
      "has_mortgage\n",
      "no     50775\n",
      "yes    49225\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- has_dependents ---\n",
      "has_dependents\n",
      "no     50710\n",
      "yes    49290\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- loan_purpose ---\n",
      "loan_purpose\n",
      "business     20359\n",
      "other        20022\n",
      "auto         19942\n",
      "education    19919\n",
      "home         19758\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- has_cosigner ---\n",
      "has_cosigner\n",
      "no     51110\n",
      "yes    48890\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "value_counts = {col: X[col].value_counts(dropna=False) for col in cat_cols}\n",
    "\n",
    "for col, vc in value_counts.items():\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Format data**\n",
    "\n",
    "Pre-process the data by encoding categorical variables with `pd.get_dummies` and converting the target variable to a probability format i.e. `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:11:55.087578Z",
     "start_time": "2026-01-19T15:11:54.964597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 17) -> (100000, 25)\n"
     ]
    }
   ],
   "source": [
    "X_dummies = pd.get_dummies(\n",
    "    X,\n",
    "    columns=cat_cols,\n",
    "    drop_first=True,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "print(X.shape, \"->\", X_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Split samples and scale data**\n",
    "\n",
    "Split the dataset into a training and a test sample using `model_selection.train_test_split` and allocate 80% of the observations to the training sample. \n",
    "\n",
    "Scale the input variables using `preprocessing.MinMaxScaler` by fitting the scaler on the training sample and applying the transformation to both the training and the test sample. Explain why input scaling is required for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:18:08.376135Z",
     "start_time": "2026-01-19T15:18:08.268761Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X_dummies, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Data loaders**\n",
    "\n",
    "Convert the training and test sets to `torch.Tensor` objects using `torch.from_numpy` and wrap them in `utils.data.TensorDataset`. Use these datasets to create `utils.data.DataLoader` instances for training and testing. Explain why `shuffle=True` is necessary for the training loader, select an appropriate batch size, and discuss the trade-offs involved in that choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T15:30:45.988548Z",
     "start_time": "2026-01-19T15:30:45.772221Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got DataFrame)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y_train_scaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y_train)\n\u001b[1;32m      4\u001b[0m y_test_scaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y_test)\n\u001b[0;32m----> 6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_train_scaled)\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_train)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m      7\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_test_scaled)\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_test)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m      8\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got DataFrame)"
     ]
    }
   ],
   "source": [
    "X_test_scaled = np.asarray(X_test_scaled)\n",
    "X_train_scaled = np.asarray(X_train_scaled)\n",
    "y_train_scaled = np.asarray(y_train)\n",
    "y_test_scaled = np.asarray(y_test)\n",
    "\n",
    "train_dataset = utils.data.TensorDataset(torch.from_numpy(X_train_scaled).float(), torch.from_numpy(y_train).float())\n",
    "test_dataset = utils.data.TensorDataset(torch.from_numpy(X_test_scaled).float(), torch.from_numpy(y_test).float())\n",
    "train_loader = utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Model structure**\n",
    "\n",
    "Define a feedforward neural network class using PyTorch. The model takes as input a feature vector and outputs a probability scores. The model consists of two hidden layers with 16 units each, each followed by a ReLU activation, and ends with a linear output layer. Instantiate the model and print the model architecture.\n",
    "\n",
    "Note: For numerical stability, PyTorch loss functions expect raw logit scores rather than probability distributions. The sigmoid transformation is applied internally within the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Model training** \n",
    "\n",
    "Define the appropriate loss function `nn.BCEWithLogitsLoss` and an optimisation algorithm (e.g. `optim.AdamW`).  Write a PyTorch training loop to estimate the model parameters using the training sample, with a maximum of 25 epochs and a learning rate of `1e-3`. Remember to move the model and the batch data to the correct device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Model performance** \n",
    "\n",
    "Write a PyTorch evaluation loop to assess the model's generalisation performance on the test sample. Interpret the results using `metrics.classification_report` and `metrics.confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Feature importance** \n",
    "\n",
    "Using the `attr.IntegratedGradients` function, compute local variable importance on the test sample. Aggregate the results across all observations and display them as a bar plot. Which variables are the most important for the classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Regularisation**\n",
    "\n",
    "Apply regularisation by configuring the optimiser's `weight_decay`, adding a penalty term to the loss function, inserting dropout layers after the activation functions, or implementing early stopping based on performance on a validation sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Model architecture tuning**\n",
    "\n",
    "Modify the model structure to improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
